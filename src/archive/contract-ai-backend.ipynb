{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Initialisation\n",
        "###### This section constructs our Python environment for running Contract AI\n",
        "###### It is necessary to run this code in order to use any proceeding sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install handler libraries\n",
        "try:\n",
        "    !pip cache purge\n",
        "    !pip install tiktoken\n",
        "    !pip install pypdf\n",
        "    !pip install alive-progress\n",
        "    !pip install langchain-openai==0.0.6\n",
        "    !pip install openai==1.0.0\n",
        "    !pip install langchain==0.1.7\n",
        "    !pip install pydantic==1.10.0\n",
        "    !pip install azure-search-documents==11.4.0\n",
        "    !pip install typing-extensions==4.2.0\n",
        "    print(\"\\nSuccessfully installed handler libraries\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"\\nError installing handler libraries\\n\")\n",
        "    print(errorMessage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Import handler libraries\n",
        "try:\n",
        "    import os\n",
        "    import io\n",
        "    import csv\n",
        "    import json\n",
        "    import numpy\n",
        "    import tempfile\n",
        "    import requests\n",
        "    import openai\n",
        "    import tiktoken\n",
        "    import hashlib\n",
        "    from alive_progress import alive_bar\n",
        "    from datetime import datetime\n",
        "    from azure.core.credentials import AzureKeyCredential\n",
        "    from azure.storage.blob import BlobServiceClient\n",
        "    from azure.search.documents import SearchClient\n",
        "    from langchain_openai import AzureOpenAI\n",
        "    from langchain_openai import AzureChatOpenAI\n",
        "    from langchain_openai import AzureOpenAIEmbeddings\n",
        "    from langchain_community.vectorstores.azuresearch import AzureSearch\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    from langchain.chains.summarize import load_summarize_chain\n",
        "    from langchain.chains import ConversationalRetrievalChain\n",
        "    from langchain.schema.document import Document\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from langchain.chains.llm import LLMChain\n",
        "    from langchain.chains import RetrievalQA\n",
        "    from azure.search.documents.indexes.models import (\n",
        "        SearchFieldDataType,\n",
        "        SearchableField,\n",
        "        SearchField,\n",
        "        SimpleField\n",
        "        )\n",
        "    from langchain.document_loaders import (\n",
        "        DirectoryLoader,\n",
        "        TextLoader,\n",
        "        JSONLoader,\n",
        "        CSVLoader,\n",
        "        PyPDFLoader,\n",
        "        Docx2txtLoader\n",
        "        )\n",
        "    print(\"Successfully imported handler libraries\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error importing handler libraries\\n\")\n",
        "    print(errorMessage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Set our reference variables\n",
        "openaiType = \"azure\"\n",
        "openaiKey = \"ec8bdbcae2794f0b86858b8f6674add1\"\n",
        "openaiBase = \"https://contract-ai-oai.openai.azure.com/\"\n",
        "openaiVersion = \"2023-12-01-preview\"\n",
        "openaiEmbeddingDeployment = \"contract-ai-oai-embedding\"\n",
        "openaiEmbeddingModel = \"text-embedding-ada-002\"\n",
        "openaiGptDeployment = \"contract-ai-oai-gpt\"\n",
        "openaiGptModel = \"gpt-4\"\n",
        "\n",
        "searchVersion = \"2023-11-01\"\n",
        "searchName = \"contract-ai-srch\"\n",
        "searchEndpoint = \"https://contract-ai-srch.search.windows.net\"\n",
        "searchIndex = \"contract-ai-srch-index\"\n",
        "searchCredential = \"Ne5GdyRY5QWeGCN6RPEZHs2khjWlPHRhK4qeJUo67rAzSeDDhnDP\"\n",
        "\n",
        "adlsConnectionString = \"DefaultEndpointsProtocol=https;AccountName=contractaist;AccountKey=fFa02I7n2sM2JPTCdrqenuEemYj+dE+krK4pK/xEmWp5ZV5Onvy/AL54IJCeDi7619AJKmFNpr2e+AStj7Zn3w==;EndpointSuffix=core.windows.net\"\n",
        "adlsContainerInput = \"landing-zone\"\n",
        "adlsFolderFilesInput = \"files/test-nuh-small/\"\n",
        "adlsFolderQuestionsInput = \"questions/test-nuh/\"\n",
        "adlsContainerOutput = \"processing-zone\"\n",
        "adlsFolderFilesOutput = \"files/test-nuh/\"\n",
        "adlsFolderQuestionsOutput = \"answers/test-nuh/answers.csv\"\n",
        "\n",
        "fileTypeMapping = {\n",
        "    \"*.txt\": TextLoader,\n",
        "    \"*.json\": JSONLoader,\n",
        "    \"*.csv\": CSVLoader,\n",
        "    \"*.pdf\": PyPDFLoader,\n",
        "    \"*.docx\": Docx2txtLoader\n",
        "    }\n",
        "\n",
        "questionPrompt = PromptTemplate.from_template(\"\"\"\n",
        "    You are a professional assistant who will be reviewing contractual documents.\n",
        "    Your response to the given question should be factual and as accurate as possible, solely based on the provided context.\n",
        "    Your response should state the answer as concisely as possible and then concisely provide any surrounding context of where the answer can be found in the document.\n",
        "    You may use synonyms of the words in the given question to help search for an answer.\n",
        "    If you cannot answer the given question with certainty then please state that fact and end your response.\n",
        "    The question you are being presented with is between the following set of exclamation points: !{question}!\n",
        "    The context which you may use to respond to this question can be found after this colon:\n",
        "    {context}\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Set our OpenAI variables\n",
        "try:\n",
        "    openai.api_type = openaiType\n",
        "    openai.api_key = openaiKey\n",
        "    openai.api_base = openaiBase\n",
        "    openai.api_version = openaiVersion\n",
        "    print(\"OpenAI environment variables initialised\\n\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error intialising OpenAI environment variables\\n\")\n",
        "    print(errorMessage + \"\\n\")\n",
        "\n",
        "# Create our embeddings model\n",
        "try:\n",
        "    embeddingsClient = AzureOpenAIEmbeddings(\n",
        "        azure_endpoint = openaiBase,\n",
        "        deployment = openaiEmbeddingDeployment,\n",
        "        model = openaiEmbeddingModel,\n",
        "        openai_api_key = openaiKey,\n",
        "        openai_api_type = openaiType,\n",
        "        openai_api_version = openaiVersion\n",
        "        )\n",
        "    print(\"Embeddings client initialised\\n\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error intialising embeddings client\\n\")\n",
        "    print(errorMessage + \"\\n\")\n",
        "\n",
        "# Create our search model\n",
        "try:\n",
        "    searchClient = SearchClient(\n",
        "        endpoint = searchEndpoint,\n",
        "        index_name = searchIndex,\n",
        "        credential = AzureKeyCredential(searchCredential)\n",
        "        )\n",
        "    print(\"Search client initialised\\n\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error intialising search client\\n\")\n",
        "    print(errorMessage + \"\\n\")\n",
        "\n",
        "# Create our vector model\n",
        "try:\n",
        "    vectorClient = AzureSearch(\n",
        "        azure_search_endpoint = searchEndpoint,\n",
        "        azure_search_key = searchCredential,\n",
        "        index_name = searchIndex,\n",
        "        embedding_function = embeddingsClient.embed_query\n",
        "        )\n",
        "    print(\"Vector client initialised\\n\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error intialising vector client\\n\")\n",
        "    print(errorMessage + \"\\n\")\n",
        "\n",
        "# Create our llm chat model\n",
        "try:\n",
        "    llmClient = AzureChatOpenAI(\n",
        "        azure_endpoint = openaiBase,\n",
        "        deployment_name = openaiGptDeployment,\n",
        "        openai_api_key = openaiKey,\n",
        "        openai_api_type = openaiType,\n",
        "        openai_api_version = openaiVersion\n",
        "        )\n",
        "    print(\"LLM client initialised\\n\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error intialising LLM client\\n\")\n",
        "    print(errorMessage + \"\\n\")\n",
        "\n",
        "# Create our retrieval chain model\n",
        "try:\n",
        "    qaClient = ConversationalRetrievalChain.from_llm(\n",
        "        llm = llmClient,\n",
        "        retriever = vectorClient.as_retriever(),\n",
        "        condense_question_prompt = questionPrompt,\n",
        "        return_source_documents = True,\n",
        "        verbose = False\n",
        "        )\n",
        "    print(\"QA client initialised\\n\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error intialising QA client\\n\")\n",
        "    print(errorMessage + \"\\n\")\n",
        "\n",
        "# Create our text splitter\n",
        "try:\n",
        "    textSplitter = RecursiveCharacterTextSplitter(chunk_size = 800, chunk_overlap = 200)\n",
        "    print(\"Text splitter initialised\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error intialising text splitter\\n\")\n",
        "    print(errorMessage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Test our connection to our model\n",
        "try:\n",
        "    embeddingsTest = embeddingsClient.embed_query(\"Ciao\")\n",
        "    if embeddingsTest:\n",
        "        print(\"Successfully tested connection\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error testing connection\\n\")\n",
        "    print(errorMessage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Index Rebuild\n",
        "###### This section deletes and rebuilds the vector database index\n",
        "###### It only needs running if documents need deleting and can otherwise be skipped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Delete our existing index\n",
        "try:\n",
        "    apiUrl = f\"https://{searchName}.search.windows.net/indexes/{searchIndex}?api-version={searchVersion}\"\n",
        "    apiHeaders = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"api-key\": searchCredential\n",
        "        }\n",
        "    apiResponse = requests.delete(apiUrl, headers = apiHeaders)\n",
        "    if apiResponse.status_code == 204:\n",
        "        print(\"Successfully deleted index\")\n",
        "    else:\n",
        "        print(\"Error deleting index\")\n",
        "        print(f\"Status code: {apiResponse.status_code}\")\n",
        "        print(f\"Response: {apiResponse.json()}\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error deleting index\\n\")\n",
        "    print(errorMessage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Rebuild our new index\n",
        "try:\n",
        "    apiUrl = f\"https://{searchName}.search.windows.net/indexes/{searchIndex}?api-version={searchVersion}\"\n",
        "    apiHeaders = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"api-key\": searchCredential\n",
        "        }\n",
        "    indexSchema = {\n",
        "        \"name\": searchIndex,\n",
        "        \"fields\": [\n",
        "            {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": True, \"searchable\": True, \"filterable\": True, \"sortable\": True, \"facetable\": True},\n",
        "            {\"name\": \"content\", \"type\": \"Edm.String\", \"searchable\": True, \"filterable\": True, \"sortable\": True, \"facetable\": True, \"analyzer\": \"standard.lucene\"},\n",
        "            {\"name\": \"content_vector\", \"type\": \"Collection(Edm.Single)\", \"searchable\": True, \"dimensions\": 1536, \"vectorSearchProfile\": \"vector-profile\"},\n",
        "            {\"name\": \"chunk_id\", \"type\": \"Edm.Int64\", \"searchable\": False, \"filterable\": True, \"sortable\": True, \"facetable\": True},\n",
        "            {\"name\": \"chunk_source\", \"type\": \"Edm.String\", \"searchable\": True, \"filterable\": True, \"sortable\": True, \"facetable\": True, \"analyzer\": \"standard.lucene\"},\n",
        "            {\"name\": \"chunk_type\", \"type\": \"Edm.String\", \"searchable\": True, \"filterable\": True, \"sortable\": True, \"facetable\": True, \"analyzer\": \"standard.lucene\"},\n",
        "            {\"name\": \"chunk_datetime\", \"type\": \"Edm.DateTimeOffset\", \"searchable\": False, \"filterable\": True, \"sortable\": True, \"facetable\": True}\n",
        "            ],\n",
        "        \"similarity\": {\"@odata.type\": \"#Microsoft.Azure.Search.BM25Similarity\", \"k1\": None, \"b\": None},\n",
        "        \"vectorSearch\": {\n",
        "            \"algorithms\": [{\"name\": \"vector-config\", \"kind\": \"hnsw\", \"hnswParameters\": {\"metric\": \"cosine\", \"m\": 4, \"efConstruction\": 400, \"efSearch\": 500}}],\n",
        "            \"profiles\": [{\"name\": \"vector-profile\", \"algorithm\": \"vector-config\"}]\n",
        "            }\n",
        "        }\n",
        "    apiResponse = requests.put(apiUrl, headers = apiHeaders, json = indexSchema)\n",
        "    if apiResponse.status_code == 201:\n",
        "        print(\"Successfully rebuilt index\")\n",
        "    else:\n",
        "        print(\"Error rebuilding index\")\n",
        "        print(f\"Status code: {apiResponse.status_code}\")\n",
        "        print(f\"Response: {apiResponse.json()}\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error rebuilding index\\n\")\n",
        "    print(errorMessage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Recreate our vector model\n",
        "try:\n",
        "    vectorClient = AzureSearch(\n",
        "        azure_search_endpoint = searchEndpoint,\n",
        "        azure_search_key = searchCredential,\n",
        "        index_name = searchIndex,\n",
        "        embedding_function = embeddingsClient.embed_query\n",
        "        )\n",
        "    print(\"Vector client re-initialised\\n\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error re-intialising vector client\\n\")\n",
        "    print(errorMessage + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Vectorisation\n",
        "###### This section translates input documents into vectors\n",
        "###### It only needs running to update the vector database and can otherwise be skipped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create our ADLS connection\n",
        "try:\n",
        "    blobClient = BlobServiceClient.from_connection_string(conn_str = adlsConnectionString)\n",
        "    containerClient = blobClient.get_container_client(adlsContainerInput)\n",
        "    print(\"Successfully connected to blob storage\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error connecting to blob storage\\n\")\n",
        "    print(errorMessage)\n",
        "\n",
        "# Define our file loading function\n",
        "def load_files(folderName):\n",
        "    blobData = []\n",
        "    for blob in containerClient.list_blobs(name_starts_with = folderName):\n",
        "        blobClient = containerClient.get_blob_client(blob.name)\n",
        "        blobData.append([blob.name.split(\"/\")[-1], io.BytesIO(blobClient.download_blob().readall())])\n",
        "    return blobData\n",
        "\n",
        "# Load our blob data to temp\n",
        "try:\n",
        "    blobData = load_files(adlsFolderFilesInput)\n",
        "    tempPdfs = []\n",
        "    tempDir = tempfile.TemporaryDirectory()\n",
        "    for i, [filename, byte_content] in enumerate(blobData):\n",
        "        filePath = os.path.join(tempDir.name, filename)\n",
        "        with open(filePath, \"wb\") as file:\n",
        "            file.write(byte_content.getbuffer())\n",
        "        tempPdfs.append(filePath)\n",
        "    if len(tempPdfs) > 0:\n",
        "        print(\"\\nSuccessfully loaded blobs\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"\\nError loading blobs\\n\")\n",
        "    print(errorMessage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define our hashing function\n",
        "def hash_text(text: str):\n",
        "    shaHash = hashlib.sha256()\n",
        "    shaHash.update(text.encode(\"utf-8\"))\n",
        "    hashedText = shaHash.hexdigest()\n",
        "    return hashedText\n",
        "\n",
        "# Vectorise our blobs\n",
        "vectorStore = []\n",
        "for pattern, loader in fileTypeMapping.items():\n",
        "    loadDir = DirectoryLoader(tempDir.name, glob = pattern, loader_cls = loader)\n",
        "    try:\n",
        "        document = loadDir.load_and_split()\n",
        "        if document:\n",
        "            print(f\"Successfully loaded {pattern[2:]} files:\")\n",
        "        documentList = set([sourceDoc.metadata[\"source\"].split(\"/\")[-1] for sourceDoc in document])\n",
        "        for item in documentList:\n",
        "            print(f\" - {item}\")\n",
        "    except Exception as errorMessage:\n",
        "        print(f\"Error loading {pattern[2:]} files\\n\")\n",
        "        print(errorMessage)\n",
        "    if documentList:\n",
        "        try:\n",
        "            chunks = textSplitter.split_documents(document)\n",
        "            if len(chunks) > 0:\n",
        "                print(f\"\\nSuccessfully chunked {pattern[2:]} files\")\n",
        "        except Exception as errorMessage:\n",
        "            print(f\"\\nError chunking {pattern[2:]} files\\n\")\n",
        "            print(errorMessage)\n",
        "        try:\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                chunkFilePath = chunk.metadata[\"source\"]\n",
        "                vectorStore.append({\n",
        "                    \"id\": hash_text(chunk.page_content),\n",
        "                    \"content\": chunk.page_content,\n",
        "                    \"content_vector\": embeddingsClient.embed_query(chunk.page_content),\n",
        "                    \"chunk_id\": i,\n",
        "                    \"chunk_source\": chunkFilePath.split(\"/\")[-1],\n",
        "                    \"chunk_type\": chunkFilePath.split(\".\")[-1],\n",
        "                    \"chunk_datetime\": datetime.utcnow()\n",
        "                    })\n",
        "            print(f\"\\nSuccessfully vectorised {pattern[2:]} files\")\n",
        "        except Exception as errorMessage:\n",
        "            print(f\"\\nError vectorising {pattern[2:]} files\\n\")\n",
        "            print(errorMessage)\n",
        "if len(vectorStore) > 0:\n",
        "    print(\"\\nSuccessfully chunked and vectorised all files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Upload our vector store list to cognitive services\n",
        "try:\n",
        "    for vectorItem in vectorStore:\n",
        "        results = searchClient.upload_documents(documents = vectorItem)\n",
        "    if results:\n",
        "        print(\"Successfully indexed vector documents\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error indexing vector documents\\n\")\n",
        "    print(errorMessage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Preset Question Chain\n",
        "###### This section queries our provided questions against the documents in the vector database\n",
        "###### Input questions should be provided in the blob stoage and output answers can be found alongside them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create our ADLS connection\n",
        "try:\n",
        "    blobClient = BlobServiceClient.from_connection_string(conn_str = adlsConnectionString)\n",
        "    containerClient = blobClient.get_container_client(adlsContainerInput)\n",
        "    print(\"Successfully connected to blob storage\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error connecting to blob storage\\n\")\n",
        "    print(errorMessage)\n",
        "\n",
        "# Define our file loading function\n",
        "def load_questions(folderName):\n",
        "    questionsInput = []\n",
        "    for blob in containerClient.list_blobs(name_starts_with = folderName):\n",
        "        blobClient = containerClient.get_blob_client(blob.name)\n",
        "        questionsInput.append([blob.name.split(\"/\")[-1], io.BytesIO(blobClient.download_blob().readall())])\n",
        "    return questionsInput\n",
        "\n",
        "# Load our questions\n",
        "try:\n",
        "    questionsInput = load_questions(adlsFolderQuestionsInput)\n",
        "    tempQuestionsList = []\n",
        "    tempQuestionsDir = tempfile.TemporaryDirectory()\n",
        "    for i, [filename, byte_content] in enumerate(questionsInput):\n",
        "        filePath = os.path.join(tempQuestionsDir.name, filename)\n",
        "        with open(filePath, \"wb\") as file:\n",
        "            file.write(byte_content.getbuffer())\n",
        "        tempQuestionsList.append(filePath)\n",
        "    questionsList = []\n",
        "    with open(tempQuestionsList[0], \"r\") as file:\n",
        "        for line in file:\n",
        "            question = line.strip()\n",
        "            questionsList.append(question)\n",
        "        file.close()\n",
        "    print(\"\\nSuccessfully loaded questions\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"\\nError loading questions\\n\")\n",
        "    print(errorMessage)\n",
        "\n",
        "# Load our file list\n",
        "try:\n",
        "    filesList = []\n",
        "    for file in tempPdfs:\n",
        "        filesList.append(file.split(\"/\")[-1])\n",
        "    print(\"\\nSuccessfully loaded file list\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"\\nError loading file list\\n\")\n",
        "    print(errorMessage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define our question response function\n",
        "def question_response_chain(question, fileName):\n",
        "    qaChain = RetrievalQA.from_chain_type(\n",
        "        llmClient,\n",
        "        retriever = vectorClient.as_retriever(\n",
        "            search_kwargs = {\n",
        "                \"k\": 10,\n",
        "                \"fetch_k\": 10,\n",
        "                \"filter\": {\"chunk_source\": f\"{fileName}\"}\n",
        "            }\n",
        "        ),\n",
        "        return_source_documents = True,\n",
        "        chain_type_kwargs = {\"prompt\": questionPrompt}\n",
        "    )\n",
        "    botMessage = qaChain({\"query\": question})\n",
        "    answer = botMessage[\"result\"]\n",
        "    textSource = botMessage[\"source_documents\"]\n",
        "    return answer, textSource"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Run question response chain\n",
        "data = []\n",
        "print(f\"Querying files:\")\n",
        "for item in filesList:\n",
        "    print(f\" - {item}\")\n",
        "for file in filesList:\n",
        "    try:\n",
        "        print(f\"\\nQuerying file: {file}\")\n",
        "        answersRow = [file]\n",
        "        with alive_bar(len(questionsList)) as bar:\n",
        "            for question in questionsList:\n",
        "                answer, textSource = question_response_chain(question, file)\n",
        "                answersRow.append(answer)\n",
        "                bar()\n",
        "        data.append(answersRow)\n",
        "    except Exception as errorMessage:\n",
        "        print(f\"\\nError querying file: {file}\\n\")\n",
        "        print(errorMessage)\n",
        "if len(data) > 0:\n",
        "    print(\"\\nSuccessfully queried all files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create our ADLS connection\n",
        "try:\n",
        "    blobServiceClient = BlobServiceClient.from_connection_string(conn_str = adlsConnectionString)\n",
        "    blobClient = blobServiceClient.get_blob_client(container = adlsContainerOutput, blob = adlsFolderQuestionsOutput)\n",
        "    print(\"Successfully connected to blob storage\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error connecting to blob storage\\n\")\n",
        "    print(errorMessage)\n",
        "\n",
        "# Output results into blob storage\n",
        "try:\n",
        "    headers = [\"File\"] + questionsList\n",
        "    with tempfile.NamedTemporaryFile(mode= \"w+\", newline= \"\", encoding= \"utf-8\", delete = False) as tempResults:\n",
        "        writer = csv.writer(tempResults)\n",
        "        writer.writerow(headers)\n",
        "        writer.writerows(data)\n",
        "        tempResultsPath = tempResults.name\n",
        "    with open(tempResultsPath, \"rb\") as results:\n",
        "        blobClient.upload_blob(results, overwrite = True)\n",
        "    print(\"\\nSuccessfully saved results\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"\\nError saving results\\n\")\n",
        "    print(errorMessage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Conversational Chain\n",
        "###### This section can be used to demonstrate the question/answer capability of Contract AI\n",
        "###### To make a query, edit line 3 with your question and then press the play button to the left of the cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Query our model for response\n",
        "chatHistory = []\n",
        "query = \"What is the average lengths of the contracts?\"\n",
        "try:\n",
        "    result = qaClient({\"question\": query, \"chat_history\": chatHistory})\n",
        "    print(f\"Question: {query}\")\n",
        "    print(f\"Answer: {result['answer']}\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error querying\\n\")\n",
        "    print(errorMessage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Alternate querying method\n",
        "query = \"What is the average lengths of the contracts?\"\n",
        "try:\n",
        "    result = vectorClient.hybrid_search(query = query, k = 3)\n",
        "    print(f\"Question: {query}\")\n",
        "    print(f\"Answer: {result[0].page_content}\")\n",
        "except Exception as errorMessage:\n",
        "    print(\"Error querying\\n\")\n",
        "    print(errorMessage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Keyword Search Chain\n",
        "###### This section contains experimental upgrades to Contract AI functionality and is not yet implemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define our keyword search function\n",
        "def keyword_search(keyword):\n",
        "    chunkSources = set()\n",
        "    chunkSummariesProcessing = {}\n",
        "    chunkSummaries = []\n",
        "    chunkContentScores = []\n",
        "    searchQuery = f\"Content: '{keyword}'\"\n",
        "    results = searchClient.search(searchQuery)\n",
        "    count = 0\n",
        "    for i in searchClient.search(searchQuery):\n",
        "        count += 1\n",
        "    with alive_bar(count) as bar:\n",
        "        for result in results:\n",
        "            chunkSource = result[\"chunk_source\"]\n",
        "            chunkContent = result[\"content\"]\n",
        "            chunkScore = result[\"@search.score\"]\n",
        "            chunkSources.add(chunkSource)\n",
        "            chunkContentScores.append({chunkSource: [chunkContent, chunkScore]})\n",
        "            if chunkSource in chunkSummariesProcessing:\n",
        "                chunkSummariesProcessing[chunkSource].append(chunkContent)\n",
        "            else:\n",
        "                chunkSummariesProcessing[chunkSource] = [chunkContent]\n",
        "            bar()\n",
        "    chunkSummaries = [\n",
        "        Document(page_content = \" \".join([chunk for chunk in chunkSummariesProcessing[key]]), metadata = {\"source\": key})\n",
        "        for key in chunkSummariesProcessing.keys()\n",
        "    ]\n",
        "    return chunkSources, chunkContentScores, chunkSummaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Provide our chunk summarisation prompt\n",
        "summarisationPromptTemplate = PromptTemplate.from_template(\"\"\"\n",
        "    You are an assistant and your task is to provide a complete and comprehensive summary.\n",
        "    The summary has to include all details and it has to be very clear.\n",
        "    The summary should be easy to understand for someone who has not read the original document.\n",
        "    To provide a better understanding of the topic, please pay close attention to numerical data such as dates, numbers, and statistics.\n",
        "    If the context is short, or if the information provided is duplicative, then please be concise with your response.\n",
        "    Use no more than five sentences in your response, using as few as possible based on your confidence in the response provided.\n",
        "    As a good assistant, memorise all these instructions and provide the summary of the following text after the colon:\n",
        "    {chunkText}\n",
        "\"\"\")\n",
        "\n",
        "# Define our summarisation client function\n",
        "def summarisation_client(promptTemplate, llmClient, chunkText):\n",
        "    llmChain = LLMChain(llm = llmClient, prompt = promptTemplate)\n",
        "    result = llmChain(chunkText)[\"text\"]\n",
        "    return result\n",
        "\n",
        "# Define our search result summarisation function\n",
        "def keyword_search_summarisation(chunkSummaries):\n",
        "    results = {}\n",
        "    with alive_bar(len(chunkSummaries))as bar:\n",
        "        for chunk in chunkSummaries:\n",
        "            chunkSource = chunk.metadata[\"source\"]\n",
        "            if chunkSource in results:\n",
        "                results[chunkSource].append(summarisation_client(summarisationPromptTemplate, llmClient, chunk.page_content))\n",
        "            else:\n",
        "                results[chunkSource] = summarisation_client(summarisationPromptTemplate, llmClient, chunk.page_content)\n",
        "            print(len(results[chunkSource]))\n",
        "            bar()\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define our search score filtering function\n",
        "def keyword_search_filter(chunkContentScores, narrowRangeThreshold = 0.5, largeChunkThreshold = 100, topPercentile = 10):\n",
        "    chunkContentScoresSorted = sorted(chunkContentScores, key = lambda i: list(i.values())[0][1], reverse = True)\n",
        "    chunkContentScoresSortedProcessing = {\n",
        "        source: [item[source] for item in chunkContentScoresSorted if source in item]\n",
        "        for source in {list(item.keys())[0] for item in chunkContentScoresSorted}\n",
        "    }\n",
        "    chunkSourceScores = {source: [chunkDetail[1] for chunkDetail in chunkDetails] for source, chunkDetails in chunkContentScoresSortedProcessing.items()}\n",
        "    chunkSourceScoresFiltered = {}\n",
        "    with alive_bar(len(chunkSourceScores)) as bar:\n",
        "        for source, scores in chunkSourceScores.items():\n",
        "            scoreMean = numpy.mean(scores)\n",
        "            scoreMedian = numpy.median(scores)\n",
        "            scoreRange = max(scores) - min(scores)\n",
        "            print(f\"{source}  -  Mean = {scoreMean}, Median = {scoreMedian}, Range = {scoreRange}\")\n",
        "            if scoreRange <= narrowRangeThreshold:\n",
        "                chunkSourceScoresFiltered[source] = [score for score in scores]\n",
        "            elif len(scores) > largeChunkThreshold:\n",
        "                scoreThreshold = numpy.percentile(scores, 100 - topPercentile)\n",
        "                chunkSourceScoresFiltered[source] = [score for score in scores if score > scoreThreshold]\n",
        "            else:\n",
        "                scoreThreshold = scoreMean\n",
        "                chunkSourceScoresFiltered[source] = [score for score in scores if score > scoreThreshold]\n",
        "            bar()\n",
        "    return chunkSourceScoresFiltered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Test the output of a keyword search\n",
        "keyword = \"What is the average lengths of the contracts?\"\n",
        "try:\n",
        "    chunkSources, chunkContentScores, chunkSummaries = keyword_search(keyword)\n",
        "    chunkSourceScoresFiltered = keyword_search_filter(chunkContentScores)\n",
        "except Exception as errorMessage:\n",
        "    print(errorMessage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Test the output of the keyword search summarisation\n",
        "try:\n",
        "    result = keyword_search_summarisation(chunkSummaries)\n",
        "    print(result)\n",
        "except Exception as errorMessage:\n",
        "    print(errorMessage)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
